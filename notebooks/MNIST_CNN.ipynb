{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten digit recognizer\n",
    "\n",
    "This notebook is an example of how we can implement deep learning models using Keras (wrapper for tensorflow).\n",
    "Here we use the famous MNIST data set. We will see how to implement: Complexified CNN: batch-norm + data augmentation + dropout. Further, we'll test transfer learning approach using vgg16 as a pretrained model.\n",
    "\n",
    " <b>Contents:</b>\n",
    "* A. Simple neuron (vanilla approach)\n",
    "* B. MLP\n",
    "* C. MLP with Dropout + BatchNorm\n",
    "* D. CNN with Dropout + BatchNorm + DataAugmentation\n",
    "\n",
    "### Importing utils and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from utils import *\n",
    "from __future__ import division, print_function\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download of Data and Viz\n",
    "### What does it look like ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIEAAAEVCAYAAAB3+fUzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f78f1895c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2,ncols=10,figsize=(20,5))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i in range(20):\n",
    "    ax[i].imshow(x_train[i],cmap = plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Simple Neuron\n",
    "Let's begin wih a simple neuron using a softmax activation function. Remember it's a vanilla approach ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories: 10\n"
     ]
    }
   ],
   "source": [
    "# download of data\n",
    "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
    "\n",
    "# obtaining a vector of 784 for the 28x28 images\n",
    "num_pixels = x_train.shape[1] * x_train.shape[2]\n",
    "x_train = x_train.reshape(x_train.shape[0], num_pixels).astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], num_pixels).astype('float32')\n",
    "\n",
    "# normalizing between 0 and 1\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "print('Number of categories:',y_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850.0\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def neuron_vanilla():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(10,activation='softmax',input_dim=num_pixels))\n",
    "    \n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = neuron_vanilla()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.5477 - acc: 0.8624 - val_loss: 0.3323 - val_acc: 0.9116\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.3242 - acc: 0.9105 - val_loss: 0.2938 - val_acc: 0.9195\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.2959 - acc: 0.9174 - val_loss: 0.2807 - val_acc: 0.9226\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.2823 - acc: 0.9210 - val_loss: 0.2728 - val_acc: 0.9239\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.2738 - acc: 0.9231 - val_loss: 0.2721 - val_acc: 0.9251\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.2682 - acc: 0.9246 - val_loss: 0.2668 - val_acc: 0.9261\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.2635 - acc: 0.9270 - val_loss: 0.2674 - val_acc: 0.9255\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s - loss: 0.2600 - acc: 0.9273 - val_loss: 0.2666 - val_acc: 0.9263\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s - loss: 0.2571 - acc: 0.9287 - val_loss: 0.2641 - val_acc: 0.9274\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s - loss: 0.2546 - acc: 0.9288 - val_loss: 0.2649 - val_acc: 0.9280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f78eab9ebe0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train, validation_data =(x_test,y_test),epochs=10,batch_size=64, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.264919473314 - accuracy:  0.928\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test,y_test,verbose=0)\n",
    "print('loss: ', scores[0],'- accuracy: ', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. MLP - first approach \n",
    "Let's opt for a simple MLP, with 2 hidden layers and 256 hidden units per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 269,322.0\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def MLP():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(256,activation='relu',input_dim=num_pixels))\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    \n",
    "    model.add(Dense(10,activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = MLP()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.2230 - acc: 0.9352 - val_loss: 0.1010 - val_acc: 0.9693\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.0845 - acc: 0.9746 - val_loss: 0.0755 - val_acc: 0.9749\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.0560 - acc: 0.9825 - val_loss: 0.0713 - val_acc: 0.9781\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.0409 - acc: 0.9870 - val_loss: 0.0880 - val_acc: 0.9737\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.0324 - acc: 0.9897 - val_loss: 0.0716 - val_acc: 0.9784\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.0234 - acc: 0.9920 - val_loss: 0.0764 - val_acc: 0.9801\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.0242 - acc: 0.9918 - val_loss: 0.0793 - val_acc: 0.9795\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.0174 - acc: 0.9942 - val_loss: 0.0921 - val_acc: 0.9762\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.0138 - acc: 0.9955 - val_loss: 0.1091 - val_acc: 0.9773\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.0152 - acc: 0.9950 - val_loss: 0.0888 - val_acc: 0.9780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f78e94a07b8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train, validation_data =(x_test,y_test),epochs=10,batch_size=64, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.0887658949246 - accuracy:  0.978\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test,y_test,verbose=0)\n",
    "print('loss: ', scores[0],'- accuracy: ', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. MLP - Dropout and Batch-Norm\n",
    "Let's opt for a Dropout and BatchNormalization for reducing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 271,370.0\n",
      "Trainable params: 270,346.0\n",
      "Non-trainable params: 1,024.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def MLP_b():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256,activation='relu', input_dim=num_pixels))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "              \n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Dense(10,activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = MLP_b()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 8s - loss: 0.2725 - acc: 0.9172 - val_loss: 0.1199 - val_acc: 0.9637\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 7s - loss: 0.1390 - acc: 0.9565 - val_loss: 0.0964 - val_acc: 0.9693\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 7s - loss: 0.1115 - acc: 0.9651 - val_loss: 0.0820 - val_acc: 0.9744\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 7s - loss: 0.0970 - acc: 0.9690 - val_loss: 0.0737 - val_acc: 0.9772\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 7s - loss: 0.0826 - acc: 0.9738 - val_loss: 0.0690 - val_acc: 0.9769\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 7s - loss: 0.0778 - acc: 0.9751 - val_loss: 0.0637 - val_acc: 0.9787\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 7s - loss: 0.0682 - acc: 0.9780 - val_loss: 0.0608 - val_acc: 0.9804\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 7s - loss: 0.0606 - acc: 0.9802 - val_loss: 0.0639 - val_acc: 0.9804\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 7s - loss: 0.0584 - acc: 0.9805 - val_loss: 0.0627 - val_acc: 0.9806\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 7s - loss: 0.0532 - acc: 0.9826 - val_loss: 0.0615 - val_acc: 0.9810\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 7s - loss: 0.0504 - acc: 0.9831 - val_loss: 0.0590 - val_acc: 0.9821\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 7s - loss: 0.0460 - acc: 0.9852 - val_loss: 0.0592 - val_acc: 0.9830\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 7s - loss: 0.0426 - acc: 0.9855 - val_loss: 0.0658 - val_acc: 0.9811\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 7s - loss: 0.0417 - acc: 0.9862 - val_loss: 0.0609 - val_acc: 0.9816\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 7s - loss: 0.0407 - acc: 0.9869 - val_loss: 0.0606 - val_acc: 0.9825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f78e8423be0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train, validation_data =(x_test,y_test),epochs=15,batch_size=64, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.0606030175272 - accuracy:  0.9825\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test,y_test,verbose=0)\n",
    "print('loss: ', scores[0],'- accuracy: ', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Complexified CNN \n",
    "With Data-Augmentation - Batch-Normalization + Dropout + Conv2D + MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping data\n",
    "Here we take the images as a matrix of shape (height, width, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Downloading data\n",
    "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
    "\n",
    "# reshaping the images for the conv2D (channels last)\n",
    "x_train = x_train.reshape(x_train.shape[0],28,28,1).astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0],28,28,1).astype('float32')\n",
    "\n",
    "# one hot encode for the categories\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_samples = x_train.shape[0]\n",
    "validation_samples = x_test.shape[0]\n",
    "\n",
    "data_gen_train = ImageDataGenerator(rescale=1./255,\n",
    "                              rotation_range=8,\n",
    "                              width_shift_range=0.1,\n",
    "                              height_shift_range=0.1,\n",
    "                              shear_range=0.3,\n",
    "                              zoom_range=0.1,\n",
    "                              horizontal_flip=False)\n",
    "\n",
    "data_gen_test = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = data_gen_train.flow(x_train, y_train, batch_size = batch_size)\n",
    "validation_generator = data_gen_test.flow(x_test, y_test, batch_size = batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 10, 10, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 6, 6, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 3, 3, 64)          256       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 175,338.0\n",
      "Trainable params: 174,442.0\n",
      "Non-trainable params: 896.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def complex_conv_model():\n",
    "    \n",
    "    # creating the convnet model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # first convo block\n",
    "    model.add(Conv2D(32,(5,5), activation='relu', input_shape = (28,28,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32,(5,5), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # two convo block\n",
    "    model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    " \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "        \n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "              \n",
    "    model.add(Dense(10,activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = complex_conv_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "937/937 [==============================] - 27s - loss: 0.5260 - acc: 0.8388 - val_loss: 0.0566 - val_acc: 0.9829\n",
      "Epoch 2/10\n",
      "937/937 [==============================] - 23s - loss: 0.1678 - acc: 0.9513 - val_loss: 0.0321 - val_acc: 0.9906\n",
      "Epoch 3/10\n",
      "937/937 [==============================] - 23s - loss: 0.1298 - acc: 0.9626 - val_loss: 0.0247 - val_acc: 0.9915\n",
      "Epoch 4/10\n",
      "937/937 [==============================] - 23s - loss: 0.1019 - acc: 0.9711 - val_loss: 0.0208 - val_acc: 0.9935\n",
      "Epoch 5/10\n",
      "937/937 [==============================] - 23s - loss: 0.0964 - acc: 0.9728 - val_loss: 0.0260 - val_acc: 0.9919\n",
      "Epoch 6/10\n",
      "937/937 [==============================] - 23s - loss: 0.0840 - acc: 0.9767 - val_loss: 0.0241 - val_acc: 0.9919\n",
      "Epoch 7/10\n",
      "937/937 [==============================] - 22s - loss: 0.0775 - acc: 0.9774 - val_loss: 0.0195 - val_acc: 0.9940\n",
      "Epoch 8/10\n",
      "937/937 [==============================] - 23s - loss: 0.0733 - acc: 0.9790 - val_loss: 0.0373 - val_acc: 0.9907\n",
      "Epoch 9/10\n",
      "937/937 [==============================] - 23s - loss: 0.0699 - acc: 0.9803 - val_loss: 0.0269 - val_acc: 0.9919\n",
      "Epoch 10/10\n",
      "937/937 [==============================] - 22s - loss: 0.0612 - acc: 0.9833 - val_loss: 0.0240 - val_acc: 0.9933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f39d4675ac8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator,\n",
    "        steps_per_epoch = train_samples // batch_size,\n",
    "        epochs=10,\n",
    "        validation_data = validation_generator,\n",
    "        validation_steps = validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Visual check on small sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(validation_generator,steps=validation_generator.n/batch_size) \n",
    "predictions = [np.argmax(i) for i in np.round(predictions)]\n",
    "y_tst = [np.argmax(i) for i in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted digitis</th>\n",
       "      <th>true digits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    predicted digitis  true digits\n",
       "0                   7            7\n",
       "1                   2            2\n",
       "2                   1            1\n",
       "3                   0            0\n",
       "4                   4            4\n",
       "5                   1            1\n",
       "6                   4            4\n",
       "7                   9            9\n",
       "8                   5            5\n",
       "9                   9            9\n",
       "10                  0            0\n",
       "11                  6            6\n",
       "12                  9            9\n",
       "13                  0            0\n",
       "14                  1            1\n",
       "15                  5            5\n",
       "16                  9            9\n",
       "17                  7            7\n",
       "18                  3            3\n",
       "19                  4            4\n",
       "20                  9            9\n",
       "21                  6            6\n",
       "22                  6            6\n",
       "23                  5            5\n",
       "24                  4            4\n",
       "25                  0            0\n",
       "26                  7            7\n",
       "27                  4            4\n",
       "28                  0            0\n",
       "29                  1            1\n",
       "30                  3            3\n",
       "31                  1            1\n",
       "32                  3            3\n",
       "33                  4            4\n",
       "34                  7            7\n",
       "35                  2            2\n",
       "36                  7            7\n",
       "37                  1            1\n",
       "38                  2            2\n",
       "39                  1            1\n",
       "40                  1            1\n",
       "41                  7            7\n",
       "42                  4            4\n",
       "43                  2            2\n",
       "44                  3            3\n",
       "45                  5            5\n",
       "46                  1            1\n",
       "47                  2            2\n",
       "48                  4            4\n",
       "49                  4            4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'true digits':y_tst[:50],\n",
    "              'predicted digitis':predictions[:50]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:deepenv]",
   "language": "python",
   "name": "conda-env-deepenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
